{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "import json\n",
    "import seaborn as sns\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('masterchef_data_cleaned.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.keys()\n",
    "US = pd.DataFrame(data['United States'])\n",
    "Canada = pd.DataFrame(data['Canada'])\n",
    "Brazil = pd.DataFrame(data['Brazil'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([US, Canada, Brazil])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winner(row):\n",
    "    return row['evaluations'][-1] == 'WINNER'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df[(df['season'] == 1) & (df['nation'] == 'United States')]['evaluations'])\n",
    "new_df = df.copy()\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['won'] = new_df.apply(winner,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = new_df[new_df['won']]\n",
    "\n",
    "# Function to generate season ID\n",
    "def generate_season_id(row):\n",
    "    country_id = {\"USA\": 1, \"Canada\": 2, \"Brazil\": 3}\n",
    "    \n",
    "    season_id = row['nation'] + str(row['season'])\n",
    "    \n",
    "    return season_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_df['seasonID'] = new_df.apply(generate_season_id, axis=1)\n",
    "winners['seasonID'] = winners.apply(generate_season_id, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['seasonID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners['seasonID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of evaluations in each season\n",
    "\n",
    "def num_episodes(row, evaluations):\n",
    "    if row['won']:\n",
    "        evaluations[row['seasonID']] = len(row['evaluations'])\n",
    "\n",
    "evaluations = {}\n",
    "for i, row in new_df.iterrows():\n",
    "    num_episodes(row, evaluations)\n",
    "print(evaluations)\n",
    "\n",
    "\n",
    "def add_num_eval(row, evaluations):\n",
    "    return evaluations[row['seasonID']]\n",
    "\n",
    "new_df['numEval'] = new_df.apply(add_num_eval, args=(evaluations,), axis=1)\n",
    "\n",
    "new_df = new_df[~new_df['won']]\n",
    "new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners['age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "ttest_ind(a=new_df['age'], b=winners['age'], equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some facts about the winners\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def eval_count(df):\n",
    "    a = []\n",
    "    for item in df['evaluations']:\n",
    "        a = np.concatenate([a, item])\n",
    "        \n",
    "    values = np.unique(a, return_counts=True)\n",
    "\n",
    "    values = pd.DataFrame({'counts':values[1], 'values': values[0]})\n",
    "    return values\n",
    "winner_values = eval_count(winners)\n",
    "all_values = eval_count(new_df)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.barplot(data=winner_values, x='counts', y='values', hue='values', legend=None)\n",
    "plt.title('Winner evaluations')\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.barplot(data=all_values, x='counts', y='values', hue='values', legend=None)\n",
    "plt.title('All other evaluations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot_data=(winner_values['counts'] + all_values['counts']).reset_index().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion(row, total_no_elim):\n",
    "    if row['values'] in ['ELIM', 'WINNER', 'RUNNER_UP(S)']:\n",
    "        return 0\n",
    "    return row['counts']/total_no_elim\n",
    "\n",
    "win_total = winner_values['counts'].sum() - winner_values[winner_values['values'].isin(['ELIM', 'WINNER', 'RUNNER_UP(S)'])]['counts'].sum()\n",
    "all_total = all_values['counts'].sum() - all_values[all_values['values'].isin(['ELIM', 'WINNER', 'RUNNER_UP(S)'])]['counts'].sum()\n",
    "\n",
    "\n",
    "all_values['proportion'] = all_values.apply(proportion, args = (all_total,), axis=1)\n",
    "winner_values['proportion'] = winner_values.apply(proportion, args = (win_total,), axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_values\n",
    "# doesn't have runner up, wdr, or ban\n",
    "winner_values = pd.concat([winner_values, pd.DataFrame.from_dict({'counts':[0, 0, 0], 'values':['BAN', 'RUNNER_UP(S)', 'WDR'], 'proportion':[0,0,0]})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_values.sort_values(by='values')['proportion'] - all_values.sort_values(by='values')['proportion']\n",
    "combined = pd.merge(winner_values, all_values, on='values', suffixes=['_win', '_all'])\n",
    "combined['prop_diff'] = combined['proportion_win'] - combined['proportion_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prop_ratio_signed(row):\n",
    "    if row['prop_diff'] == 0:\n",
    "        return 0;\n",
    "    else:\n",
    "        return row['proportion_all']/row['prop_diff']\n",
    "    \n",
    "combined['prop_ratio'] = 1-(combined['proportion_all']/combined['proportion_win'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['total'] = combined['counts_all'] + combined['counts_win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=combined, x='total', y='values', hue = 'values')\n",
    "plt.title('All value counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=combined, x='prop_ratio', y='values', hue='values', legend=None)\n",
    "plt.text(0.5, -.18, \"<-Non-winners had more | Winners had more ->\", ha='center', fontsize=10, transform=plt.gca().transAxes)\n",
    "plt.title('Winner vs all contestants proportion differences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clearest indication of a winner is definitely performance in individual challenges. Winners had individual wins as 18% of their evaluations, while the average competitor (including winners) only had 10% of their evaluations as individual wins. This means winners were nearly twice as likely as all competitors to win challenges.\n",
    "\n",
    "Interestingly, team performance was much less different. If anything, they performed slightly worse than agerage, with a slightly lower proportion of wins.\n",
    "\n",
    "high_i_w\n",
    "0.123167   0.091639\n",
    "\n",
    "win i w\n",
    "0.180352   0.100248"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict who will win on after each new evaluation. We can do this a few ways:\n",
    "\n",
    "1. Train a model for each unique number of people.\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_ind(row):\n",
    "    ind = [i for i in row['evaluations'] if i[-1] == 'I']\n",
    "    return ind\n",
    "\n",
    "winners['ind'] = winners.apply(num_ind, axis=1)\n",
    "\n",
    "def win_prop(row):\n",
    "    return len([i for i in row['ind'] if i == 'WIN_I'])/len(row['ind'])\n",
    "\n",
    "winners['win_prop'] = winners.apply(win_prop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = winners.reset_index()\n",
    "k = winners['ind'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners['win_prop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners['win_prop'].mean()\n",
    "\n",
    "# index 14 won 6/11\n",
    "# index 27 won 8/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners.loc[[14, 27]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imm_counts(row):\n",
    "    return len([i for i in row['evaluations'] if i[0:3] == \"IMM\"])\n",
    "\n",
    "immunity_counts = winners.apply(imm_counts, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_len(row):\n",
    "    return len(row['evaluations'])\n",
    "eval_length = winners.apply(eval_len, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immunity_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "win_imm_prop = immunity_counts/eval_length\n",
    "mean(win_imm_prop)\n",
    "# win_imm_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imm_counts = new_df.apply(imm_counts, axis=1)\n",
    "\n",
    "all_eval_counts = new_df.apply(eval_len, axis=1)\n",
    "\n",
    "all_imm_prop = all_imm_counts/all_eval_counts\n",
    "\n",
    "\n",
    "mean(all_imm_prop)\n",
    "# all_imm_counts/all_eval_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(all_imm_prop, win_imm_prop, equal_var=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
